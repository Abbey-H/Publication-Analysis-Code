---
title: "Data Analysis Appendix"
author: Hammell et al. 
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document: default
  word_document:
    toc: yes
    toc_depth: '4'
subtitle: Temporal Course of Over-Generalized Conditioned Threat Expectancy in Posttraumatic Stress Disorder
header-includes:
- \usepackage{parskip}
- \usepackage{setspace}
---

```{r setup, echo=FALSE, warnings = FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, warnings = FALSE)
```

\pagebreak

Please note that R code is available in the supplemental online material that outlines how to fit our SSANOVA model and compute corresponding contrasts.

### 1. Overview of Smoothing Splines

Our mixed-effects nonparametric model was fit using a smoothing spline analysis of variance (SSANOVA; Gu, 2013). The SSANOVA model estimates unknown functions (smoothing splines), which relate our response variable to the predictor variables from the sample data. Unlike parametric regression, the SSANOVA model does not assume that the relationship between the response and predictor variables follows some predetermined (parametric) form; instead, the SSANOVA approach estimates the functional form of the relationship from the data itself (Gu, 2013). This aspect of nonparametric regression increases model flexibility, which is ideal for discovering the the form of relationships among variables.

Using nonparametric regression, we can theoretically create a model that fits our sample data perfectly (i.e., has a mean-squared error of 0); however, this model with "perfect fit" will almost inevitability be more inaccurate when generalizing the model to future samples of data because it capitalizes on noise (James et al., 2013, pgs. 29-26). One critical aspect of the SSANOVA model is that it introduces a smoothing penalty to the model with the purpose of finding an ideal balance between fitting the sample data and the roughness of the estimated model function (Gu, 2013; Kimeldorf & Wahba, 1970). The variance and smoothing parameters for the present model were estimated using the two-stage approach described in Helwig (2016), which estimates the smoothing parameters via generalized cross-validation (Craven & Wahba, 1978) after estimating the variance parameters via restricted maximum likelihood estimation (Patterson & Thompson, 1971). The main purpose of the cross-validation technique is to find a model that has the best chance at replication in future samples (James et al., 2013). In other words, it aims to protect against overfitting. 

Figure A1 illustrates the above points. Below are three graphs of the same data, with the black line indicating the underlying true function of how variable *y* and *x* are related in the population; the black dots represent a random sample of data drawn from the population. Each graph shows a different colored line that is a smoothing spline model fit using a specific smoothing parameter (SP). The first model uses no smoothing parameter, resulting in a model that fits the data perfectly. The second model uses a smoothing parameter of 1, resulting in a model that is almost linear. Lastly, the third model uses cross-validation (ordinary leave-one-out (LOO) method in this case) to select the smoothing parameter, resulting in a model that is the closest to the underlying true function of the population.

\vspace{24pt}

```{r, echo = FALSE, fig.width = 7.5, fig.height = 3}
set.seed(3423)
x <- seq(0,1,length=50)
y <- sin(2*pi*x) + rnorm(50, sd =0.7)
true.y <- sin(2*pi*x)

par(mfrow=c(1,3))
#no smoother; capitalizing on variance
plot(x,y, main = "SP = 0", bty = "n")
lines(x,true.y)
rmod.norm <- smooth.spline(y ~ x, spar = 0)
lines(rmod.norm, col = "blue")
text(0.75, 1.5, labels = "A", cex = 2.5)

#extreme smoothing
plot(x,y, main = "SP = 1", bty = "n")
lines(x,true.y)
rmod.ext <- smooth.spline(y ~ x, spar = 1)
lines(rmod.ext, col = "red")
text(0.75, 1.5, labels = "B", cex = 2.5)

#cv (leave-one-out) smoother
plot(x,y, main = "SP = Selected via CV", bty = "n")
lines(x,true.y)
rmod.cv <- smooth.spline(y ~ x, cv = TRUE)
lines(rmod.cv, col = "purple")
text(0.75, 1.5, labels = "C", cex = 2.5)
```
*Figure A1.* Finding a model of the data with (A) no smoothing parameter (SP), (B) a smoothing parameter of 1, and (C) a smoothing parameter selected via cross-validation.
\pagebreak

### 2. General Model Information
We used the following model form,

\[
Risk = f(Trial,Group,Stimulus) + Subject + Error
\]

where $f(\cdot)$ is some unknown function that is to be estimated from the data, $Subject$ is a random intercept (i.e., baseline risk appraisal) that is unique to each subject (which is nested within $Group$ in the case of our model), and $Error$ is the model error term. The function $f(\cdot)$ outputs the expected $Risk$ appraisal from the input combination of $Trial$, $Group$, and $Stimulus$. 

We fit the model using the *bigssp* function in the "bigsplines" package (Helwig, 2018) in R (R Core Team, 2018). Our predictor variables include $Trial$ (Acquisition - 1-8; Generalization - 1-10), $Group$ (3 levels: PTSD, SubPTSD, trauma controls (TC)), and $Stimulus$ (Acquisition - 3 levels: $vCS-$, $oCS-$, $CS+$; Generalizaton - 6 levels: $vCS-$, $oCS-$, $GS_1$, $GS_2$, $GS_3$, $CS+$), which were modeled using cubic, nominal, and ordinal smoothing splines, respectively. See Gu (2013) and Helwig (2017) for more information on types of smoothing splines. 


#### 2.1 General Model - Statistical Details

Let $y_{its}$ denote the risk rating recorded from the $i$-th subject during the $t$-th trial of the $s$-th stimulus. In our case, we have $i \in \{1, \ldots, N\}$, $t \in \{1,\ldots, 10\}$, and $s \in \{\mbox{vCS-}, \mbox{oCS-}, \mbox{GS}_1, \mbox{GS}_2, \mbox{GS}_3, \mbox{CS+}\}$

The assumed three-way smoothing spline analysis of variance (SSANOVA) model has the form
\[
y_{its} = f(t, g_i, s) + u_i + \epsilon_{its}
\]
where $g_i \in \{\mbox{Control}, \mbox{Sub-PTSD}, \mbox{PTSD} \}$ denotes the group membership of the $i$-th subject, $u_i \sim \mbox{N}(0, \sigma_{g_i}^2)$ are idependent random intercepts with group specific variance terms, and $\epsilon_{its} \sim \mbox{N}(0, \sigma^2)$ are independent and identically distributed (iid) error terms, which are assumed to be independent of the $u_i$ terms. 

The three-way SSANOVA model decomposes the mean function such as
\[
\begin{split}
f(t, g_i, s) &= f_0 + f_T(t) + f_G(g_i) + f_S(s) + f_{TG}(t,g_i)\\
& \quad + f_{TS}(t,s) + f_{GS}(g_i, s) + f_{TGS}(t, g_i, s)
\end{split}
\]
where $f_0$ is a constant (intercept), $f_T$ is the main effect of trial, $f_G$ is the main effect of group, $f_S$ is the main effect of stimulus, $f_{TG}$ is the trial-group (two-way) interaction effect, $f_{TS}$ is the trial-stimulus (two-way) interaction effect, $f_{GS}$ is the group-stimulus (two-way) interaction effect, $f_{TGS}$ is the trial-group-stimulus (three-way) interaction effect.

#### 2.2 Function Representation of the SSANOVA
Letting $z_i = (t, g_i, s)$, the SSANOVA represents the unknown function $f(\cdot)$ such as 
\[
f(z_i) = \sum_{j=1}^p \psi_j(z_i) \beta_j
\]
where the $\psi_j(\cdot)$ are known basis functions, and the $\beta_j$ are unknown coefficients, which define the linear combination of the basis functions that produce the mean function $f(\cdot)$. This implies that we can write
\[
f(z_i) = \mathbf{x}_{i}' \boldsymbol\beta
\]
where $\mathbf{x}_{i} = [\psi_1(z_i), \ldots, \psi_p(z_i)]'$ is a $p \times 1$ vector containing the basis functions evaluated at $z_i$, and $\boldsymbol\beta = (\beta_1, \ldots, \beta_p)'$ is a $p \times 1$ vector of basis function coefficients. Note that estimating the mean function amounts to estimating the function coefficient vector $\boldsymbol\beta$. 

#### 2.3 Fitted Values of the Model
Let $\hat{\boldsymbol\beta}$ denote the estimated function coefficients, and let $\boldsymbol\Sigma_{\hat{\beta}}$ denote the covariance matrix of $\hat{\boldsymbol\beta}$. The model fitted values have the form
\[
\hat{y}_i = \mathbf{x}_i' \hat{\boldsymbol\beta}
\]
and the variance of the fitted values has the form
\[
\mbox{V}(\hat{y}_i) = \mathbf{x}_i' \mbox{V}(\hat{\boldsymbol\beta}) \mathbf{x}_i = \mathbf{x}_i' \boldsymbol\Sigma_{\hat{\beta}} \mathbf{x}_i
\]
This implies that an approximate 95\% confidence interval has the form
\[
\hat{y}_i \pm 1.96 \sqrt{ \mbox{V}(\hat{y}_i) }
\]

### 3 Contrasts
Follow-up tests on the SSANOVA results were conducted using contrasts, as outlined in Helwig, Shorter, Ma, and Hsiao-Wecksler (2016). A “*contrast*” refers to the (model-implied) difference between risk appraisals for particular combinations of predictor variables. Contrasts can be used to conduct statistical tests (e.g., *t*-tests) of the difference between risk appraisals for any combination of stimulus, group, and trial. Note that given the Bayesian interpretation to obtain the model and standard errors, we tested differences in acquistion and generalization between groups using 95% CIs. We also obtained *p*-values under the frequentist philosophy for those interested, which match results obtained by the 95% CIs.

#### 3.1 Stimulus Contrasts - Statistical Definition 
Suppose we want to examine differences between the (model predicted) risk ratings for two different stimuli as proposed above. Let $z^{s_1} = (t, g, s_1)$ and $z^{s_2} = (t, g, s_2)$ denote the covariates for two arbitrary stimuli $s_1, s_2 \in \{\mbox{vCS-}, \mbox{oCS-}, \mbox{GS}_1, \mbox{GS}_2, \mbox{GS}_3, \mbox{CS+}\}$ with $s_1 \neq s_2$. Similarly, let $\mathbf{x}^{s_1} = [\psi_1(z^{s_1}), \ldots, \psi_p(z^{s_1})]'$ and $\mathbf{x}^{s_2} = [\psi_1(z^{s_2}), \ldots, \psi_p(z^{s_2})]'$ denote the basis functions evaluated at the covariates $z^{s_1}$ and $z^{s_2}$. The estimated risk difference has the form
\[
\hat{\delta}_{s_1, s_2} = (\mathbf{x}^{s_1} - \mathbf{x}^{s_2})' \hat{\boldsymbol\beta}
\]
and the variance of the estimated risk difference has the form
\[
\mbox{V}( \hat{\delta}_{s_1, s_2} ) = (\mathbf{x}^{s_1} - \mathbf{x}^{s_2})' \boldsymbol\Sigma_{\hat{\beta}} (\mathbf{x}^{s_1} - \mathbf{x}^{s_2})
\]
This implies that an approximate 95\% confidence interval has the form
\[
\hat{\delta}_{s_1, s_2} \pm 1.96 \sqrt{ \mbox{V}( \hat{\delta}_{s_1, s_2} ) }
\]

#### 3.2 Group-Stimulus Contrasts - Statistical Definition
Suppose that we want to examine the (model predicted) group differences between stimulus contrasts. Let $z_{g_k}^{s_j} = (t, g_k, s_j)$ denote the covariate for stimulus $s_j \in \{\mbox{vCS-}, \mbox{oCS-}, \mbox{GS}_1, \mbox{GS}_2, \mbox{GS}_3, \mbox{CS+}\}$ and group $g_k \in \{\mbox{Control}, \mbox{Sub-PTSD}, \mbox{PTSD} \}$. Similarly, let $\mathbf{x}_{g_k}^{s_j} = [\psi_1(z_{g_k}^{s_j}), \ldots, \psi_p(z_{g_k}^{s_j})]'$ denote the basis functions evaluated at the covariate $z_{g_k}^{s_j}$. Given two arbitrary stimuli $(s_1, s_2)$ and groups $(g_1, g_2)$, the estimated risk difference has the form
\[
\hat{\delta}{}_{g_1, g_2}^{s_1, s_2} = (\mathbf{d}_{g_1} - \mathbf{d}_{g_2})' \hat{\boldsymbol\beta}
\]
where $\mathbf{d}_{g_k} = \mathbf{x}_{g_k}^{s_1} - \mathbf{x}_{g_k}^{s_2}$ is the $k$-th group's difference between the basis for the two stimuli for $k \in \{1,2\}$. The variance of the estimated risk difference has the form
\[
\mbox{V}( \hat{\delta}{}_{g_1, g_2}^{s_1, s_2} ) = (\mathbf{d}_{g_1} - \mathbf{d}_{g_2})' \boldsymbol\Sigma_{\hat{\beta}} (\mathbf{d}_{g_1} - \mathbf{d}_{g_2})
\]
This implies that an approximate 95\% confidence interval has the form
\[
\hat{\delta}{}_{g_1, g_2}^{s_1, s_2} \pm 1.96 \sqrt{ \mbox{V}( \hat{\delta}{}_{g_1, g_2}^{s_1, s_2} ) }
\]

#### 3.3 Generic Contrasts - Statistical Definition
If one wants to form any generic contrast of the stimuli and/or groups. This can be done by simply creating a new predictor vector $\mathbf{x}_*$ that contains the linear combination of interest. The estimates have the form
\[
\hat{\delta}_{*} = \mathbf{x}_{*}' \hat{\boldsymbol\beta}
\]
and the variance of the estimates have the form
\[
\mbox{V}( \hat{\delta}_{*} ) = \mathbf{x}_{*}' \boldsymbol\Sigma_{\hat{\beta}} \mathbf{x}_{*}
\]
This implies that an approximate 95\% confidence interval has the form
\[
\hat{\delta}_{*} \pm 1.96 \sqrt{ \mbox{V}( \hat{\delta}_{*} ) }
\]
For example, suppose that we want to examine the difference between the average of the GS responses and the vCS- response. Let $z^{\mathrm{GS}_k} = (t, g, \mbox{GS}_k)$ denote the covariate vector for the $\mbox{GS}_k$ stimuli ($k = 1,2,3$), and let $z^{\mathrm{vCS-}} = (t, g, \mbox{vCS-})$ denote the covariate vector for the $\mbox{vCS-}$ stimulus. Similarly, let $\mathbf{x}^{\mathrm{GS}_k} = [\psi_1(z^{\mathrm{GS}_k}), \ldots, \psi_p(z^{\mathrm{GS}_k})]'$ and $\mathbf{x}^{\mathrm{vCS-}} = [\psi_1(z^{\mathrm{vCS-}}), \ldots, \psi_p(z^{\mathrm{vCS-}})]'$ denote the basis functions evaluated at the covariates $z^{\mathrm{GS}_k}$ (for $k = 1,2,3$) and $z^{\mathrm{vCS-}}$, respectively. Now simply define
\[
\mathbf{x}_* = \left( \frac{1}{3} \textstyle\sum_{k=1}^3 \mathbf{x}^{\mathrm{GS}_k} \right) - \mathbf{x}^{\mathrm{vCS-}}
\]
and use the above formulas to calculate the estimated difference, the variance of the difference, and the confidence interval for the estimated difference.

\pagebreak

\begin{center}
References 
\end{center}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent

Craven, P., & Wahba, G. (1978). Smoothing noisy data with spline functions. *Numerische 	Mathematik*, *31(4)*, 377-403. 

Gu, C. (2013). *Smoothing spline anova models* (Second ed.). New York: Springer-Verlag.

Helwig, N. E. (2016). Efficient estimation of variance components in nonparametric mixed-	effects models with large samples. *Statistics and Computing*, *26(6)*, 1319-1336.

Helwig, N. E. (2017). Regression with ordered predictors via ordinal smoothing splines. *Frontiers in Applied Mathematics and Statistics*, *3*, 15.

Helwig, N. E. (2018). bigsplines: Smoothing Splines for Large Samples. R package version 	1.1-1. https://CRAN.R-project.org/package=bigsplines

Helwig, N. E., Shorter, K. A., Ma, P., & Hsiao-Wecksler, E. T. (2016). Smoothing spline 	analysis of variance models: A new tool for the analysis of cyclic biomechanical data. 	*Journal of Biomechanics*, *49*, 3216-3222. 

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An introduction to statistical learning with applications in R*. New York, NY: Springer.

Kimeldorf G, Wahba G. (1970). Spline functions and stochastic processes. *Sankhyā: The Indian Journal of Statistics, Series A*, *32*, 173-180.

Patterson, H. D., & Thompson, R. (1971). Recovery of inter-block information when block sizes are unequal. *Biometrika*, *58(3)*, 545-554.

R Core Team. (2018). *R: A language and environment for statistical computing. R Foundation for Statistical Computing* [Computer software manual]. Vienna, Austria. 	Retrieved from https://www.R-project.org/.
